The two interrelated subjects: Machine Learning and Data Mining.

Terms like “Artificial Intelligence”, “Machine Learning”, and even “Deep Learning”.

At the highest level, AI is defined as leveraging computers or machines to mimic the problem-solving and the decision-making capabilities of the human mind.

Machine Learning is a subset within AI that's more focused on the use of various self-learning algorithms that derive knowledge from data in order to predict outcomes.

And then, finally, deep learning is a further subset within even machine learning, and deep learning is often thought of as scalable machine learning because it automates a lot of the feature extraction process away and eliminates some of the human intervention involved to enable the use of some really, really big data sets.


The textbook 'Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow' covers practical aspects of Machine Learning theory utilising three primary libraries/frameworks used within the field: Scikit-learn, Keras, and Tensorflow.



Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed. —Arthur Samuel, 1959 
Machine Learning is the science (and art) of programming computers so they can learn from data. Machine learning is at the forefront of modern Artificial Intelligence and Data Science.
Machine learning: When we see any content on Youtube, Instagram, then our mobile will show the same content to us. Like if you see the contents related to pet then it will show us the advertisements related to pet items, 
And a more engineering-oriented one: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. —Tom Mitchell, 1997 
Machine learning has been around for decades in some specialized applications, such as Optical Character Recognition (OCR).
the first ML application took over the world back in the 1990s: the spam filter.

If you just download a copy of Wikipedia, your computer has a lot more data, but it is not suddenly better at any task. Thus, downloading a copy of Wikipedia is not Machine Learning.



Your spam filter is a Machine Learning program that, given examples of spam emails (e.g., flagged by users) and examples of regular (nonspam, also called “ham”) emails, can learn to flag spam. 
Machine learns itself that which emails are spams and which are not i.e. email classification.
The examples that the system uses to learn are called the training set. Each training example is called a training instance (or sample). 
In this case, the task T is to flag spam for new emails, the experience E is the training data, and the performance measure P needs to be defined; for example, you can use the ratio of correctly classified emails. This particular performance measure is called accuracy, and it is often used in classification tasks. 


In Self driving cars, we give images to machines and then they learn itself



Supervised Learning:
  The first type of ML is called “supervised learning”: Supervised learning: me ap ke pas labelled images hoti hain :  is when we use labeled data sets to train algorithms to classify data or predict outcomes. 
  Labeled data, means that the rows in the data set are labeled, tagged, or classified in some interesting way that tells us something about that data. So, it could be a yes or a no, or it could be a particular category of some different attribute.

Unsupervised learning: 
  has so many data and program will feed that data. Program will learn that data i.e., clustering data.  
  This is when we use machine learning algorithms to analyze and cluster unlabeled data sets, and this method helps us discover hidden patterns or groupings without the need for human intervention. So, we're using unlabeled data here. 
  The different techniques for unsupervised learning. One method is “clustering”. And a real-world example of this is when organizations try to do customer segmentation.

Reinforcement learning: 
  has an environment and an agent. That agent will do some work on this environment and we will learn with the environment and learn the things from that environment.  



The topics we will cover in this section:
Artificial neurons: 
  that goes all the way back to biology and how our brains work. And we're going to cover this topic called Connectionism.

We're going to look at how we can take that idea and that concept and construct and train our own neural networks to solve problems for us.

Concepts such as data selection, data wrangling and various reduction techniques, and how to improve our training of neural networks.
Things like network topologies, training and various layers and things that we can add into our network to make it better, so for improved solutions. data augmentation of existing data, how can we make our data give us more. 
the MCP neuron and the perceptron the two things
